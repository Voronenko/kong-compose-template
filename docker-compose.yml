version: '2.1'

volumes:
  kong_data: {}

networks:
  kong-net:

services:
  kong-migrations:
    image: "${KONG_DOCKER_TAG:-voronenko/kong:2.0.2-ubuntu}"
    command: kong migrations bootstrap && kong migrations finish
    depends_on:
      db:
        condition: service_healthy
    environment:
      KONG_DATABASE: postgres
      KONG_PG_DATABASE: ${KONG_PG_DATABASE:-kong}
      KONG_PG_HOST: db
      KONG_PG_PASSWORD: ${KONG_PG_PASSWORD:-kong}
      KONG_PG_USER: ${KONG_PG_USER:-kong}
    networks:
      - kong-net
    restart: on-failure

  kong-migrations-up:
    image: "${KONG_DOCKER_TAG:-voronenko/kong:2.0.2-ubuntu}"
    command: kong migrations up && kong migrations finish
    depends_on:
      db:
        condition: service_healthy
    environment:
      KONG_DATABASE: postgres
      KONG_PG_DATABASE: ${KONG_PG_DATABASE:-kong}
      KONG_PG_HOST: db
      KONG_PG_PASSWORD: ${KONG_PG_PASSWORD:-kong}
      KONG_PG_USER: ${KONG_PG_USER:-kong}
    networks:
      - kong-net
    restart: on-failure

  kong:
    image: "${KONG_DOCKER_TAG:-voronenko/kong:2.0.2-ubuntu}"
    user: "${KONG_USER:-root}"
    depends_on:
      db:
        condition: service_healthy
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: db
      KONG_PG_PORT: 5432
      KONG_PG_DATABASE: ${KONG_PG_DATABASE:-kong}
      KONG_PG_PASSWORD: ${KONG_PG_PASSWORD:-kong}
      KONG_PG_USER: ${KONG_PG_USER:-kong}

      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG:  /dev/stderr
      KONG_ADMIN_ERROR_LOG:  /dev/stderr

      KONG_PROXY_LISTEN:     0.0.0.0:8000, 0.0.0.0:8443 ssl
      KONG_ADMIN_LISTEN:     0.0.0.0:8001, 0.0.0.0:8444 ssl

      KONG_CASSANDRA_CONTACT_POINTS: db

      KONG_PLUGINS: "bundled,oidc,sa-jwt-claims-validate"
#,myplugin"
      KONG_LOG_LEVEL: debug
    networks:
      - kong-net
    ports:
      - "8000:8000/tcp" # Listener
      - "8001:8001/tcp" # Admin API
      - "8443:8443/tcp" # Listener  (SSL)
      - "8444:8444/tcp" # Admin API (SSL)
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 10s
      timeout: 10s
      retries: 10
    restart: on-failure
#    volumes:
#      - kong-plugin-myplugin:/usr/local/lib/luarocks/rocks-5.1/kong-plugin-myplugin

  db:
    image: postgres:9.5
    environment:
      POSTGRES_DB: ${KONG_PG_DATABASE:-kong}
      POSTGRES_PASSWORD: ${KONG_PG_PASSWORD:-kong}
      POSTGRES_USER: ${KONG_PG_USER:-kong}
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${KONG_PG_USER:-kong}"]
      interval: 30s
      timeout: 30s
      retries: 3
    restart: on-failure
    stdin_open: true
    tty: true
    networks:
      - kong-net
    volumes:
      - kong_data:/var/lib/postgresql/data

  #######################################
  # Konga database prepare
  #######################################
  konga-prepare:
    image: pantsel/konga:latest
    command: "-c prepare -a postgres -u postgresql://${KONG_PG_USER:-kong}:${KONG_PG_PASSWORD:-kong}@db:5432/${KONG_PG_DATABASE:-kong}"
    environment:
      DB_ADAPTER: postgres
      DB_HOST: db
      DB_USER: ${KONG_PG_USER:-kong}
      DB_PASSWORD: ${KONG_PG_PASSWORD:-kong}
      DB_DATABASE: ${KONG_PG_DATABASE:-kong}
      NODE_ENV: production
      KONGA_SEED_USER_DATA_SOURCE_FILE: /tmp/konga-bootstrap/users.js
    volumes:
      - ./konga:/tmp/konga-bootstrsap:ro
    networks:
      - kong-net
    restart: on-failure
    links:
      - db
    depends_on:
      db:
        condition: service_healthy
  #######################################
  # Konga: Kong GUI
  #######################################
  konga:
    image: pantsel/konga:latest
    restart: on-failure
    networks:
        - kong-net
    environment:
      DB_ADAPTER: postgres
      DB_HOST: db
      DB_USER: ${KONG_PG_USER:-kong}
      DB_PASSWORD: ${KONG_PG_PASSWORD:-kong}
      DB_DATABASE: ${KONG_PG_DATABASE:-kong}
      NODE_ENV: production
      KONGA_SEED_USER_DATA_SOURCE_FILE: /tmp/konga-bootstrap/users.js
    volumes:
      - ./konga:/tmp/konga-bootstrsap:ro
    depends_on:
      db:
        condition: service_healthy

    ports:
      - "1337:1337"
